---
title: "Human Activity Recognition Prediction"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Summary

In this project, the goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to build a model that accurately predicts *how well* they did a weight lifting exercise (categorical 'classe' variable).

A classification tree is built that yields very low accuracy (36%), compared to a random forest model that has 98% accuracy. To improve runtime performance in random forest, we use parallel processing, and change the default cross validation method used in caret: bootstrapping with 25 samples, to  k-fold cross validation with 5 samples. 

## The Data

### How the data was collected

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different ways: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

As per Velloso *et al.*: *"For feature extraction we used a sliding window approach with different lengths from 0.5 second to 2.5 seconds, with 0.5 second overlap. In each step of the sliding window approach we calculated features on the Euler angles (roll, pitch and yaw), as well as the raw accelerometer, gyroscope and magnetometer readings. For the Euler angles of each of the four sensors we calculated eight features: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness, generating in total 96 derived feature sets."*

### Loading packages

```{r, echo = T, message=FALSE, warning=FALSE,cache=T}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(RColorBrewer)
library(kableExtra)
library(magrittr)
library(caret)
library(xtable)
library(rpart)
library(rpart.plot)
library(parallel)
library(doParallel)
```

### Loading & exploring the data

**Loading the Data**
```{r, echo = T, cache=T}
#creating folder for data
if(!file.exists('project_data')) {
  dir.create('project_data')
}

#downloading train dataset
train_fileURL= 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
download.file(train_fileURL, destfile = './project_data/train.csv')
train = read.csv('./project_data/train.csv', na.strings=c("", "NA"))

#downloading test dataset
test_fileURL = 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'
download.file(test_fileURL, destfile = './project_data/test.csv')
test = read.csv('./project_data/test.csv', na.strings=c("", "NA"))
```

The training set has 19,622 rows and 160 variables.
```{r, echo = T, cache=T}
dim(train)
```

Having a look at the data (won't print outputs due to size),  there's a mix of factor and numeric variables, some NAs and error terms (i.e: "#DIV/0!" from cvs) that we'll need to clean up.
```{r, echo = T, results='hide', cache=T}
str(train[,1:50])
str(train[,51:100])
str(train[,101:160])
head(train)
```

### The response variable

There is a balanced number of instances of each Class in the dataset, with Class A the most frequent. For each participant, we have between 24-35% of observations of Class A measurements. 

**Number of observations by Class & Particpant**  
```{r, fig.width=10, fig.height=3, echo =FALSE, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}

p1 = ggplot(train, aes(x = classe, fill = 'red')) +
  geom_bar(colour = 'white') +
  guides(fill=FALSE) +
  labs(y="Count", x='Classe')

p2 = tableGrob(table(train$classe, train$user_name))

grid.arrange(p1, p2, nrow =1)
```

### Missing Data

In total, there are 100 variables with NAs in our training set, all of which are missing the bulk of data (19,216). Upon closer inspection, it's clear that these columsn are summaries of other rows related to changes in the window of observations (new_window variable - example below). 

**NA count per column** 
```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
missing = sort(sapply(train, function(x) sum(is.na(x))))
missing[missing>0]
```

**Example: *new_window (yes/no)* vs NAs (*TRUE*) in *max_roll_bell variable***
```{r, echo = F, message=FALSE, warning = FALSE,fig.pos='h', style='float: left', cache=T}

table(train$new_window, is.na(train$max_roll_belt)) #missing data correlated to new window variable

```

In the validation data, we will predict based on observations at a particular time point, so we will later get rid of all the above columns that are summarising one sliding window.

### Cleaning data

As well as summary columns full of NAs, we will get rid of the following that won't be useful in our prediction: 

+ **X**: row numbers
+ **user_name**: we're not interested in the participant identifier
+ **timestamps (all three columns)**: since these can't be replicated, it won't be a useful predictor for future observations (not time series)
+ **new_window**: this column is more indicative of study design and related to summary statistic columns, so we will also omit rows where new_window = 'yes'
+ **num_window**: part of study design and related to timing and order of exerises by subject

```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
train2 = train %>%
  filter(new_window == 'no') %>% 
  select_if(function(x) {!any(is.na(x))}) %>% 
  select_if(function(x) {!all(x=='')}) %>%
  select(-(X:num_window))
```

Now we have a much smaller dataset with 19,216 rows/observations, and 53 variables (from the original 160). Excluding our categorical outcome 'classe', all other variables are numeric.
```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
dim(train2)
```

## Exploratory Analysis: PCA

Running a model with all variables in the dataset will be computationally intensive. Principal component analysis (PCA) is a method of extracting important variables (in form of components) from a large set of variables available in a data set. It extracts low dimensional set of features from a high dimensional data set with the objective of capturing as much information as possible.

First, we remove 'classe' to run PCA on the rest of our numeric variables. Then, we can use prcomp(), which centers/normalizes variables to have mean equals to zero and standard deviation equals to 1.

```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
train_PCA = train2 %>% select(-classe) #remove dependent var for PCA

prin_comp = prcomp(train_PCA, scale. = T) #The base R function prcomp() is used to perform PCA. By default, it centers the variable to have mean equals to zero. With parameter scale. = T, we normalize the variables to have standard deviation equals to 1
```

The following are the resultant principal components. Points that are close together correspond to observations that have similar scores on the components displayed in the plot.
```{r, fig.width=7, fig.height=7, echo = T, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}
biplot(prin_comp, scale=0)
```

**Scree plot**  
This scree plot shows that ~30 componenents explain around 98% of the variance in the data set, so we can reduce 53 predictors to 30 without compromising on explained variance to make any modelling more manageable.
```{r, echo = T, fig.width=7, fig.height=4, message=FALSE, warning=FALSE, cache=T, results='asis'}
pr_var = prin_comp$sdev^2 #computing variance 
prop_varex = pr_var/sum(pr_var) #proportion of variance explained
plot(cumsum(prop_varex))
```

```{r, echo = T, message=FALSE, warning=FALSE, cache=T, results='hide'}
sum(prop_varex[1:20]) #91.9% variance explained by first 20 components
sum(prop_varex[1:25]) #95.5% variance explained by first 25 components
sum(prop_varex[1:30]) #97.7% variance explained by first 30 components
```

Of course, the R Caret package can do PCA preprocessing automatically!

## Modeling: Classifiction Tree & Random Forest

We will run 1) a classificition tree; and 2) a random forest model to predict 'classe', and do some model comparison.

### Classification Tree

A decision tree uses a tree structure to represent a number of possible deision paths and an outcome for each path - since our outcome variable is categorical, we will fit a classification tree.

**Creating a validation dataset**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
set.seed(123)
inTraining = createDataPartition(train2$classe, p=0.75,list=FALSE) #creates a stratified random sample of the data
train3 = train2[inTraining,]
validation = train2[-inTraining,]
```

**Fitting the model**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
fit_ct = train(classe ~., method='rpart', preProcess='pca',data=train3)
```

**Evaluate model**
The parameter “cp” in the output represents the degree of complexity, where the tree with highest accuracy had the smallest value of cp. The accucay for this model is still very low (35%).

```{r, echo = T, fig.width=6, fig.height=3, message=FALSE, warning=FALSE, cache=T}
fit_ct
```

The final Tree uses two of the pricipal components as predictors, and doesn't appear to be able to predict Class B or C at all. 
```{r, echo = T, fig.width=6, fig.height=3, message=FALSE, warning=FALSE, cache=T}
rpart.plot(fit_ct$finalModel, box.palette="RdBu", shadow.col="gray", nn=TRUE)
```

Our confusion matrix shows the average accuracy across resamples (35%) with negligible predictive power on any Class other than A. On our validation data, the accuracy is 39%: as discussed, it can't predict Class B or C, and is only successfully predicting Class A.

```{r, echo = T, fig.width=6, fig.height=3, message=FALSE, warning=FALSE, cache=T}
confusionMatrix(fit_ct)
ctPred = predict(fit_ct, validation)
confusionMatrix(ctPred, validation$classe)
```

### Random Forest

In Random Forest, we grow multiple trees as opposed to a single tree above. To classify a new Class based on attributes, each tree gives a classification and each tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest).

**Improving runtime performance**

Unless something is done to improve response time, running a random forest on our training set is too slow. To improve this we'll take the following steps as suggested by [Len Greski](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md):

+ **PCA** (as above) - could be used to reduce the number of predictors, however we'll add in as preprocessing in model.
+ **Parallel processing** - to allow to run models in parallel to obtain a manageable response time. Caret supports the parallel processing capabilities of the parallel package.
+ **Changing resampling method from default of bootstrapping to k-fold cross-validation** - to reduce number of samples against which the model (random forest) is run from 25 to 5, and to change each sample's composition from leave one out to randomly selected training folds.

In his analysis, [Len Greski](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md) shows that the 5 fold cross-validation resampling technique delivered the same accuracy as the more computationally expensive bootstrapping technique.

**Fitting the model**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
set.seed(123)

#Step1: Configure parallel processing
cluster = makeCluster(detectCores() - 1) #convention to leave 1 core for OS
registerDoParallel(cluster)

#Step 2: Configure trainControl object to change resampling method to k-fold cv & sample number to 5
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)

#Step 3: Develop training model using trainControl() object we just created
fit_rf = train(classe ~., method='rf', preProcess='pca',data=train3, trControl=fitControl)

#Step 4: De-register parallel processing cluster 
stopCluster(cluster)
registerDoSEQ()
```

**Evaluate the model**

The random forest model has been able to achieve 97% accuracy with two predictors.

```{r, echo = T, fig.width=6, fig.height=3, message=FALSE, warning=FALSE, cache=T}
fit_rf
```

The confusion matrix on resamples shows that it is much better at predicting all Classes (unlike our classification tree).

On our validation data, the accuracy is 98% - we will use this model to predict our sample of 20 observations in our final test set.
```{r, echo = T, fig.width=6, fig.height=3, message=FALSE, warning=FALSE, cache=T}
confusionMatrix(fit_rf)
rfPred = predict(fit_rf, validation)
confusionMatrix(rfPred, validation$classe)
```

### Making predictions on our test set

**Applying the same data cleaning/transformations on test data**  
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
problem_id = test$problem_id #saving case id of the 20 observatinos to predict Class for

test2 = test %>%
  filter(new_window == 'no') %>% 
  select_if(function(x) {!any(is.na(x))}) %>% 
  select_if(function(x) {!all(x=='')}) %>%
  mutate(classe = problem_id) %>% #create classe column 
  select(-(X:num_window), -problem_id)
```

**Making predictions with our random forest model**  
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
rfPred_test = as_tibble(predict(fit_rf, test2)) #saving predictions on test data
final_predictions = cbind(problem_id, rfPred_test) 
final_predictions
```

