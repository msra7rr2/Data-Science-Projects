---
title: "House Price Regression"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Summary

In this project, the goal is to use house price data (availabe through [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) from homes in Ames, Iowa) to create a model that will help predict house prices based on an original 79 house features.

Four regression models are built: 1) random forest; 2) ridge regression; 3) lasso regression; and 4) elastic net using k-fold cross validation with 5 samples. Random forest was the best performer, generating the lowest median root mean square error of all models across resamples.

## Loading packages

```{r, echo = T, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(RColorBrewer)
library(kableExtra)
library(magrittr)
library(caret)
library(xtable)
library(rpart)
library(rpart.plot)
library(parallel)
library(doParallel)
library(sqldf)
options(scipen=999) #taking off scientific notation for nicer plots
```

## The Data

The data for this project comes from Kaggle: <https://www.kaggle.com/c/house-prices-advanced-regression-techniques>. 

This dataset consists of 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. Variables range from different size measures, to quality ratings, to neighborhood.

**Importing Data**
```{r, echo = T, cache=T}
#creating folder for data
if(!file.exists('data')) {
  dir.create('data')
}

#importin training set
train <- tbl_df(read.csv("/Users/raqdom/DS_Projects/Housing_Prices/data/train.csv", stringsAsFactors=FALSE))

#importing testing set
test <- tbl_df(read.csv("/Users/raqdom/DS_Projects/Housing_Prices/data/test.csv", stringsAsFactors=FALSE))
```

**Data size & variables**  

The training set has observations for 1,460 houses, and the test for 1,469 houses. As above, there are 79 explanatory variables, one ID column, and the response variable 'SalePrice', which is missing in the test data. 
```{r, echo = T, cache=T}
dim(train)
dim(test)
```

The variables in the data are a mix of integer and character variables.
```{r, echo = T, cache=T}
str(train)
```

For data cleaning and exploratory analysis, we'll combine training & test data:
```{r, echo = T, cache=T, results = 'hide'}
test = mutate(test, SalePrice = NA) #adding Y column to match train
full = train %>% bind_rows(test) 
```

### The Response Variable

We will be trying to predict house prices ('SalePrice'). The average house price in the data is $180,921, with a median of $163,000 and a few very expensive houses.

A quick look at which numeric variables are initially correlated with SalePrice revelas that the overall quality of the house, measure of size (in square feet) for the house and garage, as well as number of rooms and bathrooms are all highly correlated with price.

**Plot & summary of SalePrice **  
```{r, fig.width=5, fig.height=5, echo =FALSE, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}

p1 <- ggplot(data = full, mapping = aes(x= SalePrice)) + 
  geom_histogram (binwidth = 10000, fill = "orange") 
summary(full$SalePrice)
p1
```

**Correlation between SalePrice & numeric variables**  
```{r, fig.width=5, fig.height=7, echo =FALSE, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}
numvars = full %>% select(which(sapply(full, is.numeric)))

cor <- round(cor(numvars, use="pairwise.complete.obs"), 2) #default it will only correlate vars without NAs, use argument tells it to ignore NAs

cor_saleprice <- as.data.frame(cor) %>% 
  select(SalePrice) %>%
  mutate(feature = as.character(rownames(cor))) %>%
  mutate(cor = SalePrice) %>%
  arrange(desc(cor)) %>%
  filter(feature != "SalePrice")

p2 <- ggplot(cor_saleprice, aes(x = reorder(feature, cor) , y = cor)) +
  geom_bar(stat = "identity", fill = "pink") +
  geom_hline(yintercept = 0.5) +
  coord_flip() +
  xlab("Feature") +
  ylab("Correlation with SalePrice")

p2
```

## Missing Data

In total, there are 34 explanatory variables with NAs in our data. Some missing the bulk of observations, and some missing only one or two.

**NA count per column** 
```{r, echo = F, message=FALSE, warning = FALSE, cache=T}
missing = sort(sapply(full, function(x) sum(is.na(x))))
missing[missing>0]
```

### Data Imputation Strategy

Depending on the variable, we will deal with them in the following ways, after having carefully looked at the data documentation: 

1. **Replace NA with 'None' where NA means feature missing**: from documenentation, bulk of NA observations are from variables were NA means that house lacks that feature. 

2. **LotFrontage (*linear feet of street connected to property*)**: imputed by predicting with LotArea with linear regression.

3. **GarageYrBlt**: where missing it will be replaced by the year the house was built.

4. **MasVnrArea & MasVnrType**: substitute with 'none' where houses don't have masonry even though documentation doesn't specifiy.

5. **Impute with mode/median**: rest of missing values will be imputed using simple mode (for character vars)/ median (for quantitative vars).

**1. Replacing NAs with 'None' where NA means feature lacking**

Documentation with factor level descriptions available [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) from Kaggle.  
```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
full <- full %>%
  mutate(PoolQC = ifelse(is.na(PoolQC), 'None', PoolQC)) %>%
  mutate(MiscFeature = ifelse(is.na(MiscFeature), 'None', MiscFeature)) %>%
  mutate(Alley = ifelse(is.na(Alley), 'None', Alley)) %>%
  mutate(Fence = ifelse(is.na(Fence), 'None', Fence)) %>%
  mutate(FireplaceQu = ifelse(is.na(FireplaceQu), 'None', FireplaceQu)) %>%
  mutate(GarageFinish = ifelse(is.na(GarageFinish), 'None', GarageFinish)) %>%
  mutate(GarageQual = ifelse(is.na(GarageQual), 'None', GarageQual)) %>%
  mutate(GarageCond = ifelse(is.na(GarageCond), 'None', GarageCond)) %>%
  mutate(GarageType = ifelse(is.na(GarageType), 'None', GarageType)) %>%
  mutate(BsmtCond = ifelse(is.na(BsmtCond), 'None', BsmtCond)) %>%
  mutate(BsmtExposure = ifelse(is.na(BsmtExposure), 'None', BsmtExposure)) %>%
  mutate(BsmtQual = ifelse(is.na(BsmtQual), 'None', BsmtQual)) %>%
  mutate(BsmtFinType2 = ifelse(is.na(BsmtFinType2), 'None', BsmtFinType2)) %>%
  mutate(BsmtFinType1 = ifelse(is.na(BsmtFinType1), 'None', BsmtFinType1))
```

Bulk of missing data is now gone. 

**NA count per column** 
```{r, echo = F, message=FALSE, warning = FALSE, cache=T}
missing_2 = sort(sapply(full, function(x) sum(is.na(x))),decreasing =TRUE)
missing_2[missing_2>0]
```

 **2. Imputing LotFrontage through simple linear regression**  
 
LotFrontage is most highly correlated with LotArea, which has no data missing. We will use this to predict NAs in LotFrontage using a simple linear regression model. 

The model would predict as follows, with and without outliers:
```{r, echo = F, message=FALSE, warning = FALSE, cache=T, results = 'hide'}
#Lot frontage most highly corrlated with LotArea which has nothing missing 
as_tibble(cor) %>% 
  select(LotFrontage) %>%
  mutate(feature = as.character(rownames(cor))) %>%
  arrange(desc(LotFrontage)) 
``` 
 
```{r,  fig.width=10, fig.height=3 ,echo = F, message=FALSE, warning = FALSE, cache=T}
#Visualise what model would look like -> lots of outliers. Remove and run model. 
p3 <- ggplot(full, aes(x = LotArea, y = LotFrontage)) + 
  geom_point() +
  geom_smooth(method='lm', formula = y ~ x)

#identifying outliers 
quantiles <- quantile(full$LotArea, probs = c(0.25,0.75))
range <- 1.5 * IQR(full$LotArea)
full_lot <- full %>% filter(LotArea > (quantiles[1] - range) & LotArea < (quantiles[2] + range))

#looks much better without outliers
p4 <- ggplot(full_lot, aes(x = LotArea, y = LotFrontage)) + 
  geom_point() +
  geom_smooth(method='lm', formula = y ~ x)

grid.arrange(p3, p4, nrow = 1)
```

We'll use the data without outliers to fit our linear model and pass our model predictions to our full dataset as follows: 
```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
#model
LotFrontageLM <- lm(LotFrontage ~ LotArea, data=full_lot)

#save predictins
Prediction <- predict(LotFrontageLM, full) 

#use prediction to replace NAs in our data
full <- full %>% 
  mutate (LotFrontageLM = Prediction) %>%
  mutate (LotFrontage = round(ifelse(is.na(LotFrontage), LotFrontageLM, LotFrontage))) %>%
  select(-LotFrontageLM)
```

**3. GarageYrBlt**

Where GarageYrBlt exists in the dataset, it is commonly (in 2,216 out of 2,919 of cases) the same as the year the house was built (YearBuilt). Therefore, where it is missing, we will impute the house year built instead.
```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
nrow(full %>% filter(GarageYrBlt == YearBuilt))
full <- full %>% mutate (GarageYrBlt = round(ifelse(is.na(GarageYrBlt), YearBuilt, GarageYrBlt)))
```

**4. MasVnrArea & MasVnrType**

These two masonry variables (area & type) seem to be missing together, so it's reasonable to conclude the house has no masonry. If both are missing, we will impute 'None' where there are missing values. 
```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
full <- full %>%
  mutate(MasVnrArea = ifelse(is.na(MasVnrArea), 0, MasVnrArea)) %>%
  mutate(MasVnrType = ifelse(MasVnrArea == 0, 'None', MasVnrType))
```

We have now dealt with the majority of missing data. 

**NA count per column** 
```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
missing_3 = sort(sapply(full, function(x) sum(is.na(x))))
missing_3[missing_3>0]
```

**5. Inputing Mode/Median for rest of variables**  

The few remaining missing values, we'll replace with mode (if factor) and median (if continuos) of column.

```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
} 

full <- full %>% 
  mutate(Exterior1st = ifelse(is.na(Exterior1st), Mode(Exterior1st), Exterior1st)) %>%
  mutate(Exterior2nd = ifelse(is.na(Exterior2nd), Mode(Exterior2nd), Exterior2nd)) %>%  
  mutate(MasVnrType = ifelse(is.na(MasVnrType), Mode(MasVnrType), MasVnrType)) %>%  
  mutate(BsmtFinSF1 = ifelse(is.na(BsmtFinSF1), median(BsmtFinSF1, na.rm = TRUE), BsmtFinSF1)) %>%  
  mutate(BsmtFinSF2 = ifelse(is.na(BsmtFinSF2), median(BsmtFinSF2, na.rm = TRUE), BsmtFinSF2)) %>%  
  mutate(BsmtUnfSF = ifelse(is.na(BsmtUnfSF), median(BsmtUnfSF, na.rm = TRUE), BsmtUnfSF)) %>%  
  mutate(TotalBsmtSF = ifelse(is.na(TotalBsmtSF), median(TotalBsmtSF, na.rm = TRUE), TotalBsmtSF)) %>%  
  mutate(Electrical = ifelse(is.na(Electrical), Mode(Electrical), Electrical)) %>%  
  mutate(KitchenQual = ifelse(is.na(KitchenQual), Mode(KitchenQual), KitchenQual)) %>%  
  mutate(GarageCars = ifelse(is.na(GarageCars), Mode(GarageCars), GarageCars)) %>%    
  mutate(GarageArea = ifelse(is.na(GarageArea), median(GarageArea, na.rm = TRUE), GarageArea)) %>%  
  mutate(SaleType = ifelse(is.na(SaleType), Mode(SaleType), SaleType)) %>%    
  mutate(Utilities = ifelse(is.na(Utilities), Mode(Utilities), Utilities)) %>%      
  mutate(BsmtFullBath = ifelse(is.na(BsmtFullBath), median(BsmtFullBath, na.rm = TRUE), BsmtFullBath)) %>%  
  mutate(BsmtHalfBath = ifelse(is.na(BsmtHalfBath), median(BsmtHalfBath, na.rm = TRUE), BsmtHalfBath)) %>%  
  mutate(Functional = ifelse(is.na(Functional), Mode(Functional), Functional)) %>%    
  mutate(MSZoning = ifelse(is.na(MSZoning), Mode(MSZoning), MSZoning))
```

**Final NA count per column** 
```{r, echo = T, message=FALSE, warning = FALSE, cache=T}
missing_4 = sort(sapply(full, function(x) sum(is.na(x))))
missing_4[missing_4>0]
```

The data has been cleaned of missing values and is now ready for analysis.

## Exploratory Analysis

Let't look at the correlation between numeric variables and those most highly correlated with SalePrice again. Later on, we'll have to get rid of highly correlated variables.  

Through visualisation, the relationship between SalePrice and these variables is very clear.

**Correlation between numeric variables**
```{r, fig.width= 8, fig.height=8, echo = F, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}
numvars2 = full %>% select(which(sapply(full, is.numeric)))
cor2 <- round(cor(numvars, use="pairwise.complete.obs"), 2) #default it will only correlate vars without NAs, use argument tells it to ignore NAs
p5 <- corrplot(cor, tl.col="black", tl.pos = "lt", tl.cex = 0.9) #correlation plot to visualise 
```

```{r, fig.width=5, fig.height=7, echo = F, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}

cor_saleprice2 <- as.data.frame(cor2) %>% 
  select(SalePrice) %>%
  mutate(feature = as.character(rownames(cor2))) %>%
  mutate(cor = SalePrice) %>%
  arrange(desc(cor)) %>%
  filter(feature != "SalePrice")

p6 <- ggplot(cor_saleprice2, aes(x = reorder(feature, cor) , y = cor)) +
  geom_bar(stat = "identity", fill = "pink") +
  geom_hline(yintercept = 0.6, color = "green") +
  #geom_hline(yintercept = 0.5, color = "red") +
  coord_flip() +
  xlab("Feature") +
  ylab("Correlation with SalePrice")
```

```{r, echo = F, message=FALSE, warning = FALSE, cache=T}
highest_cor2 <- as.data.frame(as.table(cor2)) %>%
  filter(Freq > 0.5, Freq != 1) %>%
  arrange(desc(Freq))
```


**Explanatory variable plots**  
  
    
```{r, fig.width=15, fig.height=15, echo =FALSE, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}
p7 <- ggplot(full, aes (x= factor(OverallQual), y=SalePrice)) + #Rates the overall material and finish of the house
  geom_boxplot(fill = "pink")+
  xlab("OverallQual") +
  ylab("SalePrice")

p8 <- ggplot(full, aes (x= GrLivArea, y=SalePrice)) + #Above grade (ground) living area square feet
  geom_point(fill = "pink") +
  geom_smooth(method = "lm")

p9 <- ggplot(full, aes (x= factor(GarageCars), y=SalePrice)) +  #Size of garage in car capacity
  geom_boxplot(fill = "pink") +
  xlab("GarageCars") +
  ylab("SalePrice")

p10 <- ggplot(full, aes (x= GarageArea, y=SalePrice)) + #Size of garage in square feet
  geom_point(fill = "pink") +
  geom_smooth(method = "lm")

p11 <- ggplot(full, aes (x= X1stFlrSF, y=SalePrice)) + #Square feet first floor
  geom_point(fill = "pink") +
  geom_smooth(method = "lm") +
  xlab("Square feet of first floor") +
  ylab("SalePrice")

p12 <- ggplot(full, aes (x= TotalBsmtSF, y=SalePrice)) + #Total square feet of basement area
  geom_point(fill = "pink") +
  geom_smooth(method = "lm") +
  xlab("Total square feet of basement area") +
  ylab("SalePrice")

p13 <- ggplot(full, aes (x= factor(FullBath), y=SalePrice)) + 
  geom_boxplot(fill = "pink") +
  xlab("Full bathrooms above grade") +
  ylab("SalePrice")

p14 <- ggplot(full, aes (x= factor(TotRmsAbvGrd), y=SalePrice)) + #Total rooms above grade (does not include bathrooms) 
  geom_boxplot(fill = "pink") +
  xlab("Total rooms above grade") +
  ylab("SalePrice")

p15 <- ggplot(full, aes (x= YearBuilt, y=SalePrice)) + 
  geom_point(fill = "pink") +
  geom_smooth() 

p16 <- ggplot(full, aes (x= YearRemodAdd, y=SalePrice)) + #Remodel date (same as construction date if no remodeling or additions)
  geom_point(fill = "pink") +
  geom_smooth()

grid.arrange(p7,p8,p9,p10,p11,p12,p13,p14,p15,p16, nrow=5)
```

There are also other common sense factor variables in the data that are linked to SalePrice: for example neighbourhood,  whether the house has central air or garage finish.

**Neighborhood, Central Air & Garage Finish plots**  
```{r, fig.width=15, fig.height=5, echo =FALSE, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}

p17 <- ggplot(full %>% filter(!is.na(SalePrice)), aes (x= reorder(Neighborhood, SalePrice, FUN = median), y=SalePrice)) + 
  geom_boxplot(fill = "pink") +
  xlab("Neighborhood")

p17
```
  
  
```{r, fig.width=15, fig.height=5, echo =FALSE, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}

p18 <- ggplot(full %>% filter(!is.na(SalePrice)), aes (x= reorder(CentralAir, SalePrice, FUN = median), y=SalePrice)) + 
  geom_boxplot(fill = "pink") +
  xlab("Central Air")

p19 <- ggplot(full %>% filter(!is.na(SalePrice)), aes (x= reorder(GarageFinish, SalePrice, FUN = median), y=SalePrice)) + 
  geom_boxplot(fill = "pink") +
  xlab("Garage Finish")

grid.arrange(p18, p19, nrow=1)
```

## Feature Engineering

Having explored the data, from existing variables we will create the following that might be helpful in modelling later: 

+ The total number of baths, since the data only provides basement and above ground full and half baths separately, and bathrooms seem to be quite important.

+ Total square feet of the house, since size is important and basement size is given separately in original data.

+ The age of the house at point of sale, since we know when it was built.

```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
full_2 <- full %>%
  mutate(TotalBaths = BsmtFullBath + (0.5 * BsmtHalfBath) + FullBath + (0.5 * HalfBath)) %>%
  mutate(TotalSqFeet = GrLivArea + TotalBsmtSF) %>%
  mutate(Age = YrSold - YearBuilt)
```

**New Variables vs SalePrice**
```{r, fig.width=15, fig.height=5, echo =FALSE, message=FALSE, warning=FALSE,fig.pos='h', style='float: left', cache=T}

p20 <- ggplot(full_2, aes (x= as.factor(TotalBaths), y=SalePrice)) + 
  geom_boxplot(fill = "pink") +
  xlab("Total Baths")

p21 <- ggplot(full_2, aes (x= TotalSqFeet, y=SalePrice)) +
  geom_point(fill = "pink") +
  geom_smooth(method='lm', formula = y ~ x)

p22 <- ggplot(full_2, aes (x= Age, y=SalePrice)) +
  geom_point(fill = "pink") +
  geom_smooth(method='lm', formula = y ~ x)

grid.arrange(p20, p21, p22, nrow=1)
```


## Final Data Cleaning

### Dropping highly correlated variables

We will drop highly correlated variables while keeping the ones most highly correlated with SalePrice as follows.

```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
#generate list variables that are highly correlated with each other
cor_list <- as.data.frame(as.table(cor2)) %>% 
  filter(abs(Freq) > 0.6 & abs(Freq) != 1.00) %>%
  arrange(desc(Freq))

#generate separate list of variables highly correlated with SalePrice
cor_saleprice <- as.data.frame(cor2) %>% 
  select(SalePrice) %>%
  mutate(feature = as.character(rownames(cor))) %>%
  mutate(cor = SalePrice) %>%
  arrange(desc(cor)) %>%
  filter(feature != "SalePrice")

#drop highly correlated variables & some of the direct inputs into new features, but keeping the one more highly correlated with SalePrice. Also turn character variables into factors.
full_2 <- full_2 %>%
  select(-GarageArea, -GarageYrBlt, -TotRmsAbvGrd, -GrLivArea, -TotalBsmtSF, -BsmtFullBath, -BsmtHalfBath, -FullBath, - HalfBath, -LotFrontage) %>%
  mutate_if(is.character,as.factor)
```

### Standardising data

Because some of our regression models are affected by the scale of the predictors (i.e. ridge), we will standardise our numerical explanatory variables, as the penalization will then treat different explanatory variables on a more equal footing. This means that the final fit will not depend on the predictors' scale.

```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
#separating train and test sets
train_2 <- full_2 %>% filter(!is.na(SalePrice))
test_2 <- full_2 %>% filter(is.na(SalePrice))

## standardising predictors 
preProc <- preProcess(train_2[,-71], method = c("center", "scale")) #predictors that are not numeric are ignored in the calculations.

#final train and test datasets for modelling
train_3 <- predict(preProc, train_2)
test_3 <- predict(preProc, test_2)
```

## Modelling 

For our regression exercise, we will fit four different types of models for comparison: 1) random forest, 2) ridge regression, 3) lasso regression, and 4) elastic net.

For modelling, the Caret R package will be used, as it provides a consistent interface to all of R's most powerful machine learning models and is helpful in automatically choosing the best tuning parameter values, computing the final model and evaluating the model performance using cross-validation techniques.

### Cross Validation Settings

Instead of using Caret's default settings, to estimate the accuracy of our trained models and avoid overfitting, we will use k-fold cross validation on 5 subsets.

The k-fold cross validation method splits the dataset into k-subsets, then each subset is held out while the model is trained on all other subsets. This goes on until accuracy is determine for each instance in the data, and an overall accuracy estimate is provided.

```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
#configure trainControl object with our cross-validation method
fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)
```

### Model 1: Random Forest  

Random forests train a large number of "strong" decision trees and combine their predictions through bagging. There are also two sources of "randomness" for random forests: 1) Each tree is only allowed to choose from a random subset of features to split on (leading to feature selection); and 2) Each tree is only trained on a random subset of observations (resampling).

To improve response time on random forest, we will use parallel processing. Caret supports the parallel processing capabilities of the parallel package. This  allow to run models in parallel to obtain a manageable response time. 

**Fitting the model**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
set.seed(111)

#Step1: Configure parallel processing
cluster = makeCluster(detectCores() - 1) #convention to leave 1 core for OS
registerDoParallel(cluster)

#Step 2: Build model
fit_rf = train(SalePrice ~.,
               data=train_3, 
               method='rf',
               trControl=fitControl, 
               importance=TRUE
              )

#Step 3: De-register parallel processing cluster 
stopCluster(cluster)
registerDoSEQ()
```

**Performance across resamples**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
fit_rf$resample
```

### Model 2: Ridge Regression

Penalised regression allows to create a linear regression model that is penalized for having too many variables in the model, by adding a constraint in the equation. This leads to shrink the coefficient values towards zero. This allows the less contributive variables to have a coefficient close to zero or equal zero.

Ridge regression shrinks the regression coefficients, so that variables with minor contribution to the outcome have their coefficients close to zero.

The shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called L2-norm, which is the sum of the squared coefficients. The amount of the penalty can be fine-tuned using a constant called lambda (λ). Selecting a good value for λ is critical.

**Fitting the model**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
#Step 1: setup a grid range of lambda values
lambda <- 10^seq(-3, 3, length = 100)

#Step 2: compute ridge regression:
set.seed(111)
fit_ridge <- train(SalePrice ~., 
                   data = train_3, 
                   method = 'glmnet',
                   tuneGrid = expand.grid(alpha = 0, lambda = lambda),
                   trControl = fitControl
                   )
```

**Performance across resamples**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
fit_ridge$resample
```

### Model 3: Lasso Regression

Lasso stands for Least Absolute Shrinkage and Selection Operator. It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients.

In the case of lasso regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be exactly equal to zero. This means that, lasso can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model.

As in ridge regression, selecting a good value of λ for the lasso is critical.

**Fitting the model**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
set.seed(111)

fit_lasso <- train(SalePrice ~., 
                   data = train_3, 
                   method = 'glmnet', 
                   trControl = fitControl, 
                   tuneGrid = expand.grid(alpha = 1, lambda = lambda))
```

**Performance across resamples**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
fit_lasso$resample
```

### Model 4: Elastic Net

Elastic-Net is a compromise between Lasso and Ridge - it is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).

**Fitting the model**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
set.seed(111)

fit_elastic <- train(SalePrice ~., 
                     data = train_3, 
                     method = 'glmnet', 
                     trControl = fitControl, 
                     tuneLength = 10
                     )
```

**Performance across resamples**
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
fit_elastic$resample
```

### Model Comparison & Selection

To assess the performance of our models, we will compare the root mean square error (RMSE) of each. The RMSE is the square root of the variance of the residuals. It is a measure of how close the actual observations are to the models' predictions. Lower values of RMSE indicate better fit.

The random forest model has the lowest median RMSE across resamples, so we will use this to predict on our test set and make a kaggle submission.
  
```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
models <- list(Random_Forest = fit_rf, Ridge = fit_ridge, Lasso = fit_lasso, Elastic_Net = fit_elastic)
resamples(models) %>% summary(metric = 'RMSE')
```

**Create prediction submission for kaggle with winning model**

Since the random forest model had the most predictive power, we will use it to predict the house prices in the test set and make a kaggle submission:

```{r, echo = T, message=FALSE, warning=FALSE, cache=T}
testPredictions <- cbind("Id" = test$Id, "SalePrice" = predict(fit_rf, test_3))
write.csv(testPredictions, file = 'kaggle_submission.csv', quote = FALSE, row.names = FALSE)
```


